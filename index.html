<!DOCTYPE html>
<html>

<head>
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title> Catch Me If You Can't </title>

      <!-- loading fonts -->
      <link rel="preconnect" href="https://fonts.googleapis.com">
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link href="https://fonts.googleapis.com/css2?family=Cabin&family=Roboto+Mono&display=swap" rel="stylesheet">
</head>

<body>
    <div class="title">
        <h1> <mark style="background-color: black; color: white;">_Catch Me If You Can't_</mark></h1>
    </div>
    <div class="subheading">
        <h1> An Analysis of A.I Content Detectors</h1>
    </div>
    <div class="author-name">
        <p> By: Ashima Kamra </p>
    </div>

    <section class="the-AI">
        <div id="the-AI-title"> <h2><mark style="background-color: black; color: white;"> The A.I</mark></h2> </div>
        
        <div id="the-AI-text1">
            <p> The advent of Artificial Intelligence technology this past year has proven a challenge to American educational institutions. Natural Language Processors in these technologies are designed to produce output as similar to human writing as possible.
                Thus, students have rushed to use generative A.I like chatGPT to aid in their homework assignments, and teachers have alike rushed to find ways to restrain and monitor the usage of this technology for plagiarism.</p>
        </div>
        
        <div id="the-AI-text2">
            <p>While the use of A.I in American educational institutions is no new feat, dozens of companies have similarly rushed to capitalize on the new demand, creating technologies 
                to generate resumes, develop study guides, create powerpoint presentations, and more. 
            </p>
        </div>
       
        <div id="the-AI-text3">
            <p>As such, teachers have been feeling overwhelmed with the need to restrain and monitor the usage of generative A.I for plagiarism. In January of 2023, the New York Department of Education 
                went so far as to ban usage of chatGPT in schools entirely as they believed it would detract from students meaningful learning. 
            </p>
        </div>
        
        <div id="the-AI-text4">
            <p>
                But, it seems as though a guardian angel has appeared in the form of an A.I Content Detector. These tools, of which there is a daunting amount to choose from, all promise to distinguish 
                with near perfect accuracy whether a given text is human-written or A.I generated. For educators across the county, this tool presents itself as a holy grail, but the reality is far less golden.
            </p>
        </div>
        
    </section>
    
    <section class="the-problem">
        <div id="the-problem-title"> <h2><mark style="background-color: black; color: white;"> The Problem</mark></h2> </div>
        
       <div id="the-problem-text1">
            <p> Unfortunately, there are increasing reports of false positives when these detectors flag human writing as AI - like this high school senior who was accused of likely generating her essay with chatGPT 
                by the AI writing detector from Turnitin. With so many detectors to choose from, and little evaluation or credible research conducted on these tools, teachers are left with a daunting amount of options 
                to choose from, and students are left vulnerable to a skewed system of judgment if educators choose detectors that arent accurate. </p>
            
       </div>
        
       <div id="the-problem-text2">
            <p>
                This necessitates a reevaluation of these tools and the methods used to integrate A.I into the classroom as a whole.
            </p>
       </div>
       
       <div id="the-problem-text3">
        <p>
            This research project examines the accuracy of three popular A.I Content Detectors when faced with excerpts of human writing, mixed source content, and AI-generated text. I intended to bring attention 
            to the potential harmful consequences of false claims of plagiarism, and to understand the reliability of our defense mechanisms against innovative technology.
        </p>
       </div>
        
        
    </section>

    <section class="the-background">
        <div id="the-background-title"> <h2><mark style="background-color: black; color: white;">The Background</mark></h2> </div>
        <div id="the-background-text1">
            <p>
                A.I Content Detectors have emerged recently as a method of defense against students using chatGPT in school assignments. Reports from the Washington Post, and other credible sources have found these detectors
                to be significantly unreliable, misidentifing portions of selected text. In addition to reporting false positives, these detectors have also been found to be biased against non-native english speakers, flagging their writing as A.I more often than native speakers (Liang et al).
            </p>
            <p>
                Most detectors can be classified into three categories:
            </p>
         
        </div> 
        
        <div id="the-background-text2">
            <ol>
                <div id= "list-item1"><li><strong>Watermarking algorithms</strong></li></div>
                <p> Watermarking algorithms take advantage of the modifications specifically in A.I generated text, in which random subsets of vocabulary are “watermarked”. These watermarks are undetectable to humans, and can be found using specific  algorithms. There are no commercial watermarking 
                    algorithms on the market but researchers like Kirchbauer et al have developed some to use for their work. </p>
                
                <div id= "list-item2"><li><strong>Statistical outlier detection methods</strong></li></div>
                <p> Statistical outlier detection algorithms aim to distinguish between human written and AI generated based on outliers in specific qualities like burstiness (consistency in style and tone through the text) or perplexity (having an AI model to generate the next word and compare that 
                    generated to the next word in the text). As such, they use A.I algorithms themselves. Three of the four detectors I will be testing use methods similar to this.</p>
                
                <div id= "list-item3"><li><strong>Classifiers that discriminate AI generated from human written.</strong></li></div>
                <p> Classifiers are language models themselves that are trained on large datasets of both AI generated and human-written text to be able to distinguish between the two. The only available text classifier, OpenAI Text Classifier, I intended to use was taken down as of July 20th, 2023, 
                    due to “its low rate of accuracy.”</p>
            </ol>
        </div>

        <div id="the-background-text3">
            <p>
                The three A.I content detectors I will be evaluating are: GPTZero, Winston A.I, and Originality.ai. These three detectors were chosen based on various criteria. Either they are easily accessible on the internet by teachers, they have been cited as being used often by educators, or they have 
                been used in other studies testing the reliability of A.I. 
            </p>
        </div>

        
       
    </section>

    <section class="the-research-stage1">
        <div id="the-research-title"> <h2><mark style="background-color: black; color: white;"> The Research: Stage 1</mark></h2> </div>
        <div id="the-paragraphs">
            <div id="the-research-paragraph1">
                <p> <a href="https://gptzero.me/">GPTZero</a> is a detector developed by Princeton University student, Edward Tian. It is a statistical 
                    outlier detector that returns a document-level score, “completely_generated_prob” that specifies the 
                    probability the entire document was AI-generated. More information about the specifics of its technology 
                    can be found at <a href="https://gptzero.me/technology">their webpage</a>. GPTZero recommend multiple ways to reduce the risk of A.I misuse, including: “set 
                    expectations for your students that you will be checking the work through an AI detector like GPTZero, to deter misuse of AI.”
                </p>
            </div>
            <div id="the-research-paragraph2">
                <p>
                    <a href=" https://gowinston.ai/">Winston A.I</a> is a detector developed by a Canadian company with the intention of detecting content from multiple 
                    LLMs like Claude and GPT-4. It seems to be a statistical outlier detector, but the details published by Winston 
                    AI are vague and unclear. It returns a percentage of odds that the text is generated by human or A.I. In regards
                    to AI misuse, they urge teachers that “while technology can be helpful in identifying potential issues, building trust 
                    and rapport with your students can encourage them to be honest and genuine in their academic endeavors.”
                </p> 
            </div>   
            <div id="the-research-paragraph3">     
                <p>
                    <a href="https://originality.ai/">Originality.ai</a> is a detector developed by a company of the same name, claiming itself to be the “Most Accurate AI Content 
                    Detector.” Using software from Google, it has developed technology combining the classifier method and statistical outlier
                    detector. It returns a percentage of how much AI is written and how much is human generated. ... Theyve said anything 
                    explicitly about the role of AI detector in education.
                </p>
            </div>
        </div>
        

        
        <div id="the-research-text1">
            <p> Below is a comparison table of the 3 detectors with information about their payment plans, and accuracy rates, and limitations.</p>
        </div>
        <div id="the-research-table">
            <table>
                <tr>
                    <th>&nbsp;</th>
                    <th><strong>GPTZero</strong></th>
                    <th><strong>Winston A.I</strong></th>
                    <th><strong>Originality.ai</strong></th>
                </tr>
                <tr>
                    <th>Pricing Plans</th>
                    <td>Free Plan, Educator Plan: $9.99/month, Pro Plan: $19.99/month</td>
                    <td>Essential Plan: $12/month, Growth Plan: $18/month </td>
                    <td>Pay-as-you-go: $30 flat rate, Base Subscription: $14.95/month</td>
                </tr>
                <tr>
                    <th>Self-Reported Accuracy Rates</th>
                    <td>At threshold of 0.65 - 85% true positive rate, and 99% true negative rate.</td>
                    <td>Unclear. 99% accuracy on front page, 94% on FAQ pages</td>
                    <td>~94% accuracy on website. 1.5-2.5% false positive rate. </td>
                </tr>
                <tr>
                    <th>Limitations</th>
                    <td>Works better on larger text samples. Was trained with a focus on English prose.</td>
                    <td>Works better on larger text samples. Up to 2,000 word scan with the free plan. </td>
                    <td>Works better on larger text samples. Strange formatting of text can skew results. </td>
                </tr>
            </table>
        </div>
        
    </section>

    <section class="the-research-stage2">
        <div id="the-research-title2"> <h2><mark style="background-color: black; color: white;"> The Research: Stage 2</mark></h2> </div>

        <div id="the-research2-text1">
            <p>After identifying which detectors to test, which AI bots to use (ChatGPT, Bard, and Claude) and conducting an extensive literature review, I began my primary research by reaching out to potential 
                research participants and developing the writing prompt to provide them. These 7 participants were all native English speakers, aged 18-25 and attending 
                or recently graduated from higher education institutes. Their responses were additionally limited to being english prose, about a non-personal/sensitive 
                subject, and being ~200 words long. These criteria were intended to limit any A.I or personal bias</p>
        </div>

        <div id="the-research2-text2">
            <p>They each responded to this prompt regarding a <a href=https://www.nytimes.com/2021/02/05/world/asia/china-masculinity-schoolboys.html> NYT article about P.E Classes in China:</a> </p>
            <p> <mark style="background-color: white; color: black;">[<strong>Write a 200-word paragraph summarizing and analyzing this article about the Chinese education ministrys proposition to redesign school P.E classes. What was the motivation behind this proposal? What was the public response? Are any arguments made or presented by the article?</strong>] </mark></p>
        </div>

        <div id="the-research2-text3">
            <p> With each human response I got, I generated 7 mixed-source writing samples, by taking the first 100 words from each response and feeding it to 1 of 3 LLMs to generate the last 100 words using a prompt like this: </p>
            <p> <mark style="background-color: white; color: black;"> <strong> Using this paragraph - [first 100 words from human sample] - make a 200 word response to summarizing and analyzing this article about the Chinese education ministry’s proposition to redesign school P.E classes. What was the motivation behind this proposal? What was the public response? Are any arguments made or presented by the article? Use the paragraph 
                excerpt above word-for-word as the first half of your response. </strong> </mark>            
            </p>
        </div>

    
    </section>

    <section class="the-findings1">
        <div id="the-findings-title"> <h2><mark style="background-color: black; color: white;"> The Findings - GPTZero</mark></h2> </div>
        <div id="the-findings-text1">
            <p>After putting the 17 responses into each detectors, the results were as follow:</p>
        </div>

        <div id="data-viz-1">
            <img src="img/data-viz01.png" alt="GPTZero Visualization" width="500" height="500">
        </div>

        <div id="the-finding-text2">
            <p> 
                Note that each detector utilized different scales to report their findings, with some using hard numbers and others using qualitative descriptions.  Let’s first look at GPTZero:
            </p>
            <p>
                The results are scattered. For the human writing samples, there are no false positives, but the number “unclears” was 1 less than than the number of human classifications. The 
                mixed-source samples have more misclassifications. With 50% of the text being human and 50% AI, the most accurate classification would be “may include parts written by AI.” However 
                only half of the classifications were like that. For the A.I samples, once again, the general trend of the classifications leans towards A.I written, but uncertainty remains.
            </p>
               
        </div>
    </section>

    <section class="the-findings2">
        <div id="the-findings-title2"> <h2><mark style="background-color: black; color: white;"> The Findings - Winston A.I</mark></h2> </div>
        <div id="the-findings-text3">
            <p>Note that these graphs were made in Google Colab using Matplotlib.</p>
        </div>

        <div id="data-viz-2">
            <img src="img/data-viz02.png" alt="Winston A.I Visualization" width="500" height="500">
        </div>

        <div id="the-finding-text4">
            <p>
                Now, lets look at Winston A.I:
            </p>
            <p>
                For Winston A.I you can see similar results, in each categories most of the results group towards an accurate percentage. But, having 2/7 of the classifications return false positives 
                for the human samples is dangerous to say the least. This number of false positives increases in the mixed-source samples, with 3/7 being classified as 75-100% human. The AI samples seem 
                to be classified the most accurately of the 3 categories. 
            </p>
        </div>
    </section>
    
    <section class="the-findings3">
        <div id="the-findings-title3"> <h2><mark style="background-color: black; color: white;"> The Findings - Originality A.I</mark></h2> </div>
        
        <div id="data-viz-3">
            <img src="img/data-viz-03.png" alt="Originality.ai Visualization" width="500" height="500">
        </div>

        <div id="the-finding-text5">
            <p>
                Finally, lets look at Originality.ai:
            </p>
            <p>
                Originality.ai was the trickiest of the 3 to interpret, largely because it outputed 2-3 data points for each sample of writing, as opposed to 
                one like the others. The first percentage is a general confidence percentage, the number in parentheses are the confidence percentage for sections of the texts it highlights. 
            </p>
            <p>
                The percentages were varied and there was no clean, even way to divide them. As such, in order to make the results more understandable, I developed qualitative “buckets” to group 
                the different results into, using the scale from that GPTZero used.
            </p>
            <p>
                For example: I determined that if Originality.ai returned 100% AI (100% chance AI generated), it would be likely entirely AI written.
            </p>
        </div>
    </section>

    
    
    <section id="so what">
        <h2> So What? </h2>
    </section>

</body>

</html>